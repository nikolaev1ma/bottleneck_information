\section{Заключение}
Сделаем выводы о проведенных экспериментах. Изначально мы пришли к выводу что задача обучения нейронной сети это решение оптимизационной задачи
\begin{gather}
\begin{aligned}
min_{Z} I[X, Z] - \beta * I[Z, Y]
\end{aligned}
\end{gather}
То есть мы хотим минимизировать взаимную информацию между входным  пространством и латентным, при этом максимизовать общую информацию между выходом и латентым пространством. Изначально мы хотели проверить насколько эта гипотеза верна на практике. \\
Тезисно приведем выводы которые были сделаны на протяжение работы:
\begin{enumerate}
\item Рассмотрев слои, был сделан вывод, что значения по отдельным признакам зависят друг от друга, также значения по каждому признаку распредедены нестандартно, поэтому сузим пространства до дискретных и вероятностнь будем оценивать количественно.
\item Сужение должно быть оптимальным, а именно быть не инъективнм, чтобы не быть равномернораспределеным и изменятся от эпохи к эпохе, при этом сохранять информацию о изначальном пространстве.
\item Для сужений принято решение использовать конкатенацию пуллингов и дискретизацию, при этом на выходном слое использовать softmax.
\item Доказано что при сужении не достаточно использовать только дискретизацию, так как размерность полученных пространств слишком большая из-за чего отображение получается инъективно и операция построения вероятностоного пространства слишком долгая.
\item В качестве операции понижения размерности пуллинги сохраняют информацию, так как операция пуллинг часто используется как слои в нейронных сетях для понижения размерности, соответственно через них проходит нужжная информация.
\item Всего в сети 3 латентных пространства, которые последовательно упрощаются, чем сложнее пространство тем более непредстказуемо ведут себя информационные метрики на них.
\item Финальное латентное пространство подтверждает выдвинутую в начале гипотезу и для этого пространства метрика $I[X, Z]$ уменьшается, а $I[Y, Z]$ увеличивается с эпохой обучения, а значит общая метрика (27) (Information Bottleneck) также будет уменьшаться с эпохой.
\item При этом информационнык метрики на первых латентных пространствах ведут себя непредсказуемо. Если уменьшать пространство немного, то метрики либо меняются очень слабо либо хаотично, что говорит о инъективности сужения. При этом сжав слишком сильно пространство все информационные метрики: $I[X, Z]$, $I[Y, Z]$, $H[Z]$ увеличиваются с эпохой обучения.
\item Одна из причин поведения в предъиущем пункте - несохранения распределения значений при пуллинге, то есть при обучении латентное пространство становится более равномерно распределенным, отчешл все метрики возрастают, данная проблема требует дополнительного исследования.
\end{enumerate}

В данной работе были подробно рассмотрены поведения информационных метрик в динамике обучения модели, в частности была подробна рассмотрена небольшая свертночная нейронная сеть, подробно рассмотрено поведение различных слоев данной сети и статистически оценено с вероятностной точки зрения. Также в данной работе построен универсальный способ подсчета информации отдельных слоев так и совместной информации для любого состояния любой нейронной сети. Была четко поставлена гипотеза, построенная на теоретических знаниях из релевантной литературы и проверена в действии на практике.

